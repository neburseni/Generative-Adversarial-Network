{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim \n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "channels = 3\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # Size : 512 x 4 x 4\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # Size : 256 x 8 x 8\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # Size : 128 x 16 x 16\n",
    "            nn.ConvTranspose2d(128, channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "            # Size : 1 x 32 x 32\n",
    "        )\n",
    "        \n",
    "    def forward (self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Size : 1 x 32 x 32\n",
    "            nn.Conv2d(channels, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Size : 128 x 16 x 16\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Size 256 x 8 x 8\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # Size : 512 x 4 x 4\n",
    "            nn.Conv2d(512, 1 , 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()            \n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator()\n",
    "disc = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainLoader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data', train=True, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                       transforms.Resize(32),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (6): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (9): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (10): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put on GPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "gen.to(device)\n",
    "disc.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = optim.Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "desc_optimizer = optim.Adam(disc.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], Step [1], disc_loss: 2.7418, gen_loss: 1.5480, D(x): 0.89, D(G(z)): 0.23\n",
      "Epoch [0], Step [201], disc_loss: 0.1501, gen_loss: 4.7465, D(x): 0.95, D(G(z)): 0.01\n",
      "Epoch [0], Step [401], disc_loss: 0.5548, gen_loss: 4.5999, D(x): 0.95, D(G(z)): 0.02\n",
      "Epoch [0], Step [601], disc_loss: 0.5684, gen_loss: 5.7115, D(x): 0.97, D(G(z)): 0.01\n",
      "Epoch [1], Step [1], disc_loss: 0.3719, gen_loss: 3.3883, D(x): 0.81, D(G(z)): 0.06\n",
      "Epoch [1], Step [201], disc_loss: 0.2520, gen_loss: 3.8270, D(x): 0.89, D(G(z)): 0.04\n",
      "Epoch [1], Step [401], disc_loss: 0.3673, gen_loss: 4.1812, D(x): 0.83, D(G(z)): 0.03\n",
      "Epoch [1], Step [601], disc_loss: 0.2087, gen_loss: 2.8964, D(x): 0.85, D(G(z)): 0.10\n",
      "Epoch [2], Step [1], disc_loss: 0.2712, gen_loss: 3.5839, D(x): 0.86, D(G(z)): 0.04\n",
      "Epoch [2], Step [201], disc_loss: 1.7683, gen_loss: 1.0918, D(x): 0.24, D(G(z)): 0.46\n",
      "Epoch [2], Step [401], disc_loss: 0.1659, gen_loss: 2.9390, D(x): 0.95, D(G(z)): 0.07\n",
      "Epoch [2], Step [601], disc_loss: 0.5879, gen_loss: 4.0339, D(x): 0.72, D(G(z)): 0.03\n",
      "Epoch [3], Step [1], disc_loss: 0.1294, gen_loss: 5.7370, D(x): 0.89, D(G(z)): 0.01\n",
      "Epoch [3], Step [201], disc_loss: 0.4168, gen_loss: 4.1052, D(x): 0.68, D(G(z)): 0.03\n",
      "Epoch [3], Step [401], disc_loss: 0.4721, gen_loss: 5.9589, D(x): 0.97, D(G(z)): 0.00\n",
      "Epoch [3], Step [601], disc_loss: 0.1635, gen_loss: 3.9853, D(x): 0.97, D(G(z)): 0.03\n",
      "Epoch [4], Step [1], disc_loss: 0.2974, gen_loss: 5.0238, D(x): 0.77, D(G(z)): 0.01\n",
      "Epoch [4], Step [201], disc_loss: 0.2527, gen_loss: 3.9303, D(x): 0.96, D(G(z)): 0.03\n",
      "Epoch [4], Step [401], disc_loss: 0.0182, gen_loss: 5.0191, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [4], Step [601], disc_loss: 0.1641, gen_loss: 5.3651, D(x): 0.86, D(G(z)): 0.01\n",
      "Epoch [5], Step [1], disc_loss: 0.2305, gen_loss: 5.8220, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [5], Step [201], disc_loss: 0.0306, gen_loss: 4.3329, D(x): 0.99, D(G(z)): 0.03\n",
      "Epoch [5], Step [401], disc_loss: 0.0651, gen_loss: 3.2940, D(x): 0.97, D(G(z)): 0.06\n",
      "Epoch [5], Step [601], disc_loss: 0.7945, gen_loss: 2.0881, D(x): 0.55, D(G(z)): 0.21\n",
      "Epoch [6], Step [1], disc_loss: 0.4351, gen_loss: 4.7518, D(x): 1.00, D(G(z)): 0.02\n",
      "Epoch [6], Step [201], disc_loss: 0.1033, gen_loss: 4.4304, D(x): 0.99, D(G(z)): 0.02\n",
      "Epoch [6], Step [401], disc_loss: 0.1848, gen_loss: 2.4057, D(x): 0.86, D(G(z)): 0.14\n",
      "Epoch [6], Step [601], disc_loss: 0.0364, gen_loss: 5.0109, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [7], Step [1], disc_loss: 7.3904, gen_loss: 3.7357, D(x): 0.01, D(G(z)): 0.07\n",
      "Epoch [7], Step [201], disc_loss: 0.2225, gen_loss: 4.4236, D(x): 0.88, D(G(z)): 0.04\n",
      "Epoch [7], Step [401], disc_loss: 0.0435, gen_loss: 4.9370, D(x): 0.97, D(G(z)): 0.01\n",
      "Epoch [7], Step [601], disc_loss: 1.8478, gen_loss: 9.5536, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [8], Step [1], disc_loss: 0.0822, gen_loss: 4.4564, D(x): 0.96, D(G(z)): 0.02\n",
      "Epoch [8], Step [201], disc_loss: 0.0756, gen_loss: 3.8310, D(x): 0.97, D(G(z)): 0.05\n",
      "Epoch [8], Step [401], disc_loss: 0.2587, gen_loss: 4.6703, D(x): 0.94, D(G(z)): 0.03\n",
      "Epoch [8], Step [601], disc_loss: 0.1218, gen_loss: 2.9184, D(x): 1.00, D(G(z)): 0.09\n",
      "Epoch [9], Step [1], disc_loss: 0.1412, gen_loss: 4.9262, D(x): 0.99, D(G(z)): 0.01\n",
      "Epoch [9], Step [201], disc_loss: 0.0725, gen_loss: 4.8632, D(x): 0.98, D(G(z)): 0.02\n",
      "Epoch [9], Step [401], disc_loss: 0.1925, gen_loss: 4.2325, D(x): 0.93, D(G(z)): 0.03\n",
      "Epoch [9], Step [601], disc_loss: 0.0905, gen_loss: 4.4642, D(x): 0.98, D(G(z)): 0.02\n",
      "Epoch [10], Step [1], disc_loss: 0.1139, gen_loss: 4.4568, D(x): 0.96, D(G(z)): 0.02\n",
      "Epoch [10], Step [201], disc_loss: 0.4536, gen_loss: 5.0519, D(x): 0.95, D(G(z)): 0.01\n",
      "Epoch [10], Step [401], disc_loss: 0.0567, gen_loss: 4.7605, D(x): 0.99, D(G(z)): 0.02\n",
      "Epoch [10], Step [601], disc_loss: 0.3808, gen_loss: 2.2175, D(x): 0.79, D(G(z)): 0.20\n",
      "Epoch [11], Step [1], disc_loss: 0.1219, gen_loss: 2.3055, D(x): 0.99, D(G(z)): 0.18\n",
      "Epoch [11], Step [201], disc_loss: 0.1284, gen_loss: 3.2088, D(x): 0.97, D(G(z)): 0.08\n",
      "Epoch [11], Step [401], disc_loss: 1.9535, gen_loss: 5.0979, D(x): 0.90, D(G(z)): 0.03\n",
      "Epoch [11], Step [601], disc_loss: 0.2310, gen_loss: 3.9604, D(x): 0.91, D(G(z)): 0.04\n",
      "Epoch [12], Step [1], disc_loss: 0.2029, gen_loss: 3.6972, D(x): 0.96, D(G(z)): 0.04\n",
      "Epoch [12], Step [201], disc_loss: 0.1134, gen_loss: 3.8331, D(x): 0.93, D(G(z)): 0.04\n",
      "Epoch [12], Step [401], disc_loss: 0.0421, gen_loss: 5.0990, D(x): 0.98, D(G(z)): 0.01\n",
      "Epoch [12], Step [601], disc_loss: 0.2359, gen_loss: 4.3078, D(x): 0.82, D(G(z)): 0.03\n",
      "Epoch [13], Step [1], disc_loss: 0.0606, gen_loss: 4.8836, D(x): 0.98, D(G(z)): 0.01\n",
      "Epoch [13], Step [201], disc_loss: 0.0635, gen_loss: 5.0309, D(x): 0.98, D(G(z)): 0.01\n",
      "Epoch [13], Step [401], disc_loss: 0.0423, gen_loss: 4.1433, D(x): 1.00, D(G(z)): 0.04\n",
      "Epoch [13], Step [601], disc_loss: 0.0895, gen_loss: 3.9584, D(x): 0.96, D(G(z)): 0.05\n",
      "Epoch [14], Step [1], disc_loss: 0.2266, gen_loss: 5.4241, D(x): 0.98, D(G(z)): 0.01\n",
      "Epoch [14], Step [201], disc_loss: 0.0903, gen_loss: 3.1366, D(x): 0.93, D(G(z)): 0.08\n",
      "Epoch [14], Step [401], disc_loss: 0.5329, gen_loss: 1.1875, D(x): 0.64, D(G(z)): 0.39\n",
      "Epoch [14], Step [601], disc_loss: 0.1266, gen_loss: 4.3980, D(x): 0.96, D(G(z)): 0.02\n",
      "Epoch [15], Step [1], disc_loss: 0.2429, gen_loss: 5.5865, D(x): 0.82, D(G(z)): 0.01\n",
      "Epoch [15], Step [201], disc_loss: 0.1381, gen_loss: 3.1094, D(x): 0.98, D(G(z)): 0.08\n",
      "Epoch [15], Step [401], disc_loss: 0.1855, gen_loss: 2.6932, D(x): 0.93, D(G(z)): 0.11\n",
      "Epoch [15], Step [601], disc_loss: 0.1455, gen_loss: 4.4329, D(x): 0.97, D(G(z)): 0.02\n",
      "Epoch [16], Step [1], disc_loss: 0.3502, gen_loss: 4.2901, D(x): 0.77, D(G(z)): 0.03\n",
      "Epoch [16], Step [201], disc_loss: 0.4430, gen_loss: 4.5660, D(x): 0.77, D(G(z)): 0.03\n",
      "Epoch [16], Step [401], disc_loss: 0.0686, gen_loss: 4.0557, D(x): 0.96, D(G(z)): 0.03\n",
      "Epoch [16], Step [601], disc_loss: 0.3588, gen_loss: 4.1964, D(x): 0.77, D(G(z)): 0.06\n",
      "Epoch [17], Step [1], disc_loss: 0.1695, gen_loss: 5.3503, D(x): 0.92, D(G(z)): 0.01\n",
      "Epoch [17], Step [201], disc_loss: 0.2018, gen_loss: 3.6360, D(x): 0.85, D(G(z)): 0.06\n",
      "Epoch [17], Step [401], disc_loss: 0.0077, gen_loss: 6.5023, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [17], Step [601], disc_loss: 0.1014, gen_loss: 3.2223, D(x): 0.99, D(G(z)): 0.07\n",
      "Epoch [18], Step [1], disc_loss: 0.3354, gen_loss: 4.4112, D(x): 0.93, D(G(z)): 0.02\n",
      "Epoch [18], Step [201], disc_loss: 0.1083, gen_loss: 4.0826, D(x): 0.97, D(G(z)): 0.04\n",
      "Epoch [18], Step [401], disc_loss: 0.1439, gen_loss: 2.7444, D(x): 0.98, D(G(z)): 0.10\n",
      "Epoch [18], Step [601], disc_loss: 0.1204, gen_loss: 3.8161, D(x): 0.96, D(G(z)): 0.05\n",
      "Epoch [19], Step [1], disc_loss: 0.1192, gen_loss: 6.8997, D(x): 0.96, D(G(z)): 0.00\n",
      "Epoch [19], Step [201], disc_loss: 0.1656, gen_loss: 4.6427, D(x): 0.94, D(G(z)): 0.02\n",
      "Epoch [19], Step [401], disc_loss: 0.0462, gen_loss: 4.1076, D(x): 0.97, D(G(z)): 0.04\n",
      "Epoch [19], Step [601], disc_loss: 0.9714, gen_loss: 4.2348, D(x): 0.98, D(G(z)): 0.06\n"
     ]
    }
   ],
   "source": [
    "for epoch in range (20):\n",
    "    # Put batch into GPU\n",
    "    for i, (input_batch, _) in enumerate (trainLoader,0):\n",
    "        input_batch = input_batch.to(device)\n",
    "        #=======================#\n",
    "        #==Train discriminator==#\n",
    "        #=======================#\n",
    "        \n",
    "        disc.zero_grad()\n",
    "    \n",
    "        # Loss on real data\n",
    "        real_desc = disc(input_batch)\n",
    "        desc_loss_real = criterion(real_desc, torch.ones_like(real_desc))\n",
    "    \n",
    "        # Loss on fake\n",
    "        noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
    "        gen_images = gen(noise)\n",
    "        gen_desc = disc(gen_images)\n",
    "        desc_loss_fake = criterion(gen_desc, torch.zeros_like(gen_desc))\n",
    "    \n",
    "        desc_loss = desc_loss_fake + desc_loss_real\n",
    "        desc_loss.backward()\n",
    "        desc_optimizer.step()\n",
    "         \n",
    "         #=======================#\n",
    "         #==Train generator==#\n",
    "         #=======================#\n",
    "    \n",
    "        gen.zero_grad()\n",
    "                               \n",
    "        noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
    "        gen_imgs = gen(noise)\n",
    "        gen_desc = disc(gen_imgs)\n",
    "        gen_loss = criterion(gen_desc, torch.ones_like(gen_desc))\n",
    "            \n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "                               \n",
    "        if i % 200 == 0:\n",
    "            print('Epoch [{}], Step [{}], disc_loss: {:.4f}, gen_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}'\n",
    "             .format(epoch, i+1, desc_loss.item(), gen_loss.item(),\n",
    "                real_desc.mean().item(), gen_desc.mean().item()))\n",
    "\n",
    "    disp_imgs = (gen_imgs + 1.) / 2\n",
    "    torchvision.utils.save_image(disp_imgs, './img_epoch%.i.png' % epoch)                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_imgs = (gen_imgs + 1.) / 2\n",
    "torchvision.utils.save_image(disp_imgs, './img_epoch%.i.png' % epoch)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
